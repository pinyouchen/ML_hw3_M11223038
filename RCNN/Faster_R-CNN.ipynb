{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import torchvision\n",
    "from torchvision.models.detection.faster_rcnn import FastRCNNPredictor\n",
    "import torch.optim as optim\n",
    "import pytesseract\n",
    "import numpy as np\n",
    "from sklearn.metrics import precision_recall_fscore_support"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContainerDataset(Dataset):\n",
    "    def __init__(self, img_dir, annotations_dir, transform=None):\n",
    "        self.img_dir = img_dir\n",
    "        self.annotations_dir = annotations_dir\n",
    "        self.transform = transform\n",
    "        self.imgs = [f for f in sorted(os.listdir(img_dir)) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        self.annotations = [f for f in sorted(os.listdir(annotations_dir)) if f.endswith('.xml')]\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = os.path.join(self.img_dir, self.imgs[idx])\n",
    "        annotation_path = os.path.join(self.annotations_dir, self.annotations[idx])\n",
    "        \n",
    "        img = Image.open(img_path).convert(\"RGB\")\n",
    "        tree = ET.parse(annotation_path)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        boxes = []\n",
    "        for member in root.findall('object'):\n",
    "            xmin = int(member.find('bndbox').find('xmin').text)\n",
    "            ymin = int(member.find('bndbox').find('ymin').text)\n",
    "            xmax = int(member.find('bndbox').find('xmax').text)\n",
    "            ymax = int(member.find('bndbox').find('ymax').text)\n",
    "            boxes.append([xmin, ymin, xmax, ymax])\n",
    "        \n",
    "        boxes = torch.as_tensor(boxes, dtype=torch.float32)\n",
    "        labels = torch.ones((boxes.shape[0],), dtype=torch.int64)  # only one class (container)\n",
    "\n",
    "        image_id = torch.tensor([idx])\n",
    "        area = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        iscrowd = torch.zeros((boxes.shape[0],), dtype=torch.int64)\n",
    "        \n",
    "        target = {}\n",
    "        target[\"boxes\"] = boxes\n",
    "        target[\"labels\"] = labels\n",
    "        target[\"image_id\"] = image_id\n",
    "        target[\"area\"] = area\n",
    "        target[\"iscrowd\"] = iscrowd\n",
    "        \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "        \n",
    "        return img, target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "\n",
    "train_dataset = ContainerDataset(\"D:/test3/container_dataset/train/images\", \"D:/test3/container_dataset/train_xml\", transform=transform)\n",
    "val_dataset = ContainerDataset(\"D:/test3/container_dataset/val/images\", \"D:/test3/container_dataset/val_xml\", transform=transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=2, shuffle=True, collate_fn=lambda x: tuple(zip(*x)))\n",
    "val_loader = DataLoader(val_dataset, batch_size=2, shuffle=False, collate_fn=lambda x: tuple(zip(*x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): BackboneWithFPN(\n",
       "    (body): IntermediateLayerGetter(\n",
       "      (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
       "      (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "      (relu): ReLU(inplace=True)\n",
       "      (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
       "      (layer1): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "            (1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(64, eps=0.0)\n",
       "          (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer2): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(128, eps=0.0)\n",
       "          (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer3): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (3): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (4): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (5): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(256, eps=0.0)\n",
       "          (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(1024, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (layer4): Sequential(\n",
       "        (0): Bottleneck(\n",
       "          (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "          (downsample): Sequential(\n",
       "            (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
       "            (1): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          )\n",
       "        )\n",
       "        (1): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "        (2): Bottleneck(\n",
       "          (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn1): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "          (bn2): FrozenBatchNorm2d(512, eps=0.0)\n",
       "          (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "          (bn3): FrozenBatchNorm2d(2048, eps=0.0)\n",
       "          (relu): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (fpn): FeaturePyramidNetwork(\n",
       "      (inner_blocks): ModuleList(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (1): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (2): Conv2dNormActivation(\n",
       "          (0): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "        (3): Conv2dNormActivation(\n",
       "          (0): Conv2d(2048, 256, kernel_size=(1, 1), stride=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (layer_blocks): ModuleList(\n",
       "        (0-3): 4 x Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "        )\n",
       "      )\n",
       "      (extra_blocks): LastLevelMaxPool()\n",
       "    )\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(256, 3, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(256, 12, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0', '1', '2', '3'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=12544, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=2, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=8, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def get_model_instance_segmentation(num_classes):\n",
    "    model = torchvision.models.detection.fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "    in_features = model.roi_heads.box_predictor.cls_score.in_features\n",
    "    model.roi_heads.box_predictor = FastRCNNPredictor(in_features, num_classes)\n",
    "    return model\n",
    "\n",
    "model = get_model_instance_segmentation(num_classes=2)\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, optimizer, data_loader, device, epoch, print_freq):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    for i, (images, targets) in enumerate(data_loader):\n",
    "        images = list(image.to(device) for image in images)\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        loss_dict = model(images, targets)\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        losses.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += losses.item()\n",
    "        if i % print_freq == 0:\n",
    "            print(f\"Epoch [{epoch}], Iteration [{i}], Loss: {losses.item():.4f}\")\n",
    "\n",
    "    print(f\"Epoch [{epoch}] completed. Average Loss: {running_loss / len(data_loader):.4f}\")\n",
    "\n",
    "def compute_iou(box, boxes):\n",
    "    x1 = np.maximum(box[0], boxes[:, 0])\n",
    "    y1 = np.maximum(box[1], boxes[:, 1])\n",
    "    x2 = np.minimum(box[2], boxes[:, 2])\n",
    "    y2 = np.minimum(box[3], boxes[:, 3])\n",
    "\n",
    "    inter_area = np.maximum(x2 - x1, 0) * np.maximum(y2 - y1, 0)\n",
    "    box_area = (box[2] - box[0]) * (box[3] - box[1])\n",
    "    boxes_area = (boxes[:, 2] - boxes[:, 0]) * (boxes[:, 3] - boxes[:, 1])\n",
    "\n",
    "    union_area = box_area + boxes_area - inter_area\n",
    "    iou = inter_area / union_area\n",
    "    return iou\n",
    "\n",
    "def evaluate(model, data_loader, device):\n",
    "    model.eval()\n",
    "    true_positive = 0\n",
    "    false_positive = 0\n",
    "    false_negative = 0\n",
    "\n",
    "    all_true_boxes = []\n",
    "    all_pred_boxes = []\n",
    "    all_scores = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for target, output in zip(targets, outputs):\n",
    "                true_boxes = target['boxes'].cpu().numpy()\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "\n",
    "                all_true_boxes.append(true_boxes)\n",
    "                all_pred_boxes.append(pred_boxes)\n",
    "                all_scores.append(pred_scores)\n",
    "\n",
    "                for pred_box, pred_score in zip(pred_boxes, pred_scores):\n",
    "                    if pred_score > 0.5:  # 設定置信度閾值\n",
    "                        if len(true_boxes) == 0:\n",
    "                            false_positive += 1\n",
    "                        else:\n",
    "                            ious = compute_iou(pred_box, true_boxes)\n",
    "                            max_iou = np.max(ious)\n",
    "                            if max_iou > 0.5:  # IOU 閾值\n",
    "                                true_positive += 1\n",
    "                                true_boxes = np.delete(true_boxes, np.argmax(ious), axis=0)\n",
    "                            else:\n",
    "                                false_positive += 1\n",
    "\n",
    "                false_negative += len(true_boxes)\n",
    "\n",
    "    precision = true_positive / (true_positive + false_positive) if (true_positive + false_positive) > 0 else 0\n",
    "    recall = true_positive / (true_positive + false_negative) if (true_positive + false_negative) > 0 else 0\n",
    "    f1_score = 2 * (precision * recall) / (precision + recall) if (precision + recall) > 0 else 0\n",
    "\n",
    "    # Calculate mAP\n",
    "    average_precisions = []\n",
    "    for true_boxes, pred_boxes, scores in zip(all_true_boxes, all_pred_boxes, all_scores):\n",
    "        ap = calculate_ap(true_boxes, pred_boxes, scores, iou_threshold=0.5)\n",
    "        average_precisions.append(ap)\n",
    "    mAP = np.mean(average_precisions)\n",
    "\n",
    "    return precision, recall, f1_score, mAP\n",
    "\n",
    "def calculate_ap(true_boxes, pred_boxes, scores, iou_threshold=0.5):\n",
    "    sorted_indices = np.argsort(-scores)\n",
    "    pred_boxes = pred_boxes[sorted_indices]\n",
    "    scores = scores[sorted_indices]\n",
    "\n",
    "    matched = np.zeros(len(true_boxes), dtype=bool)\n",
    "    tp = np.zeros(len(pred_boxes), dtype=bool)\n",
    "    fp = np.zeros(len(pred_boxes), dtype=bool)\n",
    "\n",
    "    for i, pred_box in enumerate(pred_boxes):\n",
    "        if len(true_boxes) == 0:\n",
    "            fp[i] = True\n",
    "            continue\n",
    "\n",
    "        ious = compute_iou(pred_box, true_boxes)\n",
    "        max_iou_idx = np.argmax(ious)\n",
    "        if ious[max_iou_idx] > iou_threshold and not matched[max_iou_idx]:\n",
    "            tp[i] = True\n",
    "            matched[max_iou_idx] = True\n",
    "        else:\n",
    "            fp[i] = True\n",
    "\n",
    "    tp = np.cumsum(tp)\n",
    "    fp = np.cumsum(fp)\n",
    "    recalls = tp / len(true_boxes)\n",
    "    precisions = tp / (tp + fp)\n",
    "\n",
    "    precisions = np.concatenate(([1.0], precisions, [0.0]))\n",
    "    recalls = np.concatenate(([0.0], recalls, [1.0]))\n",
    "    for i in range(len(precisions) - 1, 0, -1):\n",
    "        precisions[i - 1] = np.maximum(precisions[i - 1], precisions[i])\n",
    "\n",
    "    indices = np.where(recalls[1:] != recalls[:-1])[0]\n",
    "    ap = np.sum((recalls[indices + 1] - recalls[indices]) * precisions[indices + 1])\n",
    "    return ap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 訓練模型\n",
    "optimizer = optim.SGD([p for p in model.parameters() if p.requires_grad], lr=0.005, momentum=0.9, weight_decay=0.0005)\n",
    "lr_scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=3, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Iteration [0], Loss: 1.5064\n",
      "Epoch [0], Iteration [50], Loss: 0.0945\n",
      "Epoch [0], Iteration [100], Loss: 0.0720\n",
      "Epoch [0], Iteration [150], Loss: 0.0509\n",
      "Epoch [0], Iteration [200], Loss: 0.0461\n",
      "Epoch [0], Iteration [250], Loss: 0.0664\n",
      "Epoch [0], Iteration [300], Loss: 0.0461\n",
      "Epoch [0], Iteration [350], Loss: 0.0401\n",
      "Epoch [0], Iteration [400], Loss: 0.0472\n",
      "Epoch [0], Iteration [450], Loss: 0.0265\n",
      "Epoch [0], Iteration [500], Loss: 0.0297\n",
      "Epoch [0], Iteration [550], Loss: 0.0219\n",
      "Epoch [0], Iteration [600], Loss: 0.0371\n",
      "Epoch [0], Iteration [650], Loss: 0.0231\n",
      "Epoch [0], Iteration [700], Loss: 0.0257\n",
      "Epoch [0], Iteration [750], Loss: 0.0291\n",
      "Epoch [0], Iteration [800], Loss: 0.0137\n",
      "Epoch [0], Iteration [850], Loss: 0.0292\n",
      "Epoch [0], Iteration [900], Loss: 0.0172\n",
      "Epoch [0], Iteration [950], Loss: 0.0135\n",
      "Epoch [0], Iteration [1000], Loss: 0.0590\n",
      "Epoch [0], Iteration [1050], Loss: 0.0217\n",
      "Epoch [0] completed. Average Loss: 0.0467\n",
      "Precision: 0.9453, Recall: 1.0000, F1-Score: 0.9719, mAP: 1.0000\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 1\n",
    "for epoch in range(num_epochs):\n",
    "    train_one_epoch(model, optimizer, train_loader, device, epoch, print_freq=50)\n",
    "    lr_scheduler.step()\n",
    "    precision, recall, f1_score, mAP = evaluate(model, val_loader, device=device)\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}, mAP: {mAP:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"best_model.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'boxes': tensor([[ 831.7161,   23.5173, 1034.7919,  103.0599]], device='cuda:0'), 'labels': tensor([1], device='cuda:0'), 'scores': tensor([0.9994], device='cuda:0')}]\n"
     ]
    }
   ],
   "source": [
    "# 加載模型\n",
    "model.load_state_dict(torch.load(\"best_model.pth\"))\n",
    "model.eval()\n",
    "\n",
    "# 在新圖像上進行推理\n",
    "img, _ = val_dataset[0]\n",
    "with torch.no_grad():\n",
    "    prediction = model([img.to(device)])\n",
    "\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9856, Recall: 1.0000, F1-Score: 0.9928\n"
     ]
    }
   ],
   "source": [
    "# 計算性能指標\n",
    "def calculate_metrics(model, data_loader, device, iou_threshold=0.5):\n",
    "    model.eval()\n",
    "    all_labels = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, targets in data_loader:\n",
    "            images = list(img.to(device) for img in images)\n",
    "            outputs = model(images)\n",
    "\n",
    "            for target, output in zip(targets, outputs):\n",
    "                true_boxes = target['boxes'].cpu().numpy()\n",
    "                true_labels = target['labels'].cpu().numpy()\n",
    "\n",
    "                pred_boxes = output['boxes'].cpu().numpy()\n",
    "                pred_scores = output['scores'].cpu().numpy()\n",
    "                pred_labels = output['labels'].cpu().numpy()\n",
    "\n",
    "                matched_gt = set()\n",
    "                for pred_box, pred_score, pred_label in zip(pred_boxes, pred_scores, pred_labels):\n",
    "                    if pred_score > iou_threshold:\n",
    "                        if len(true_boxes) > 0:\n",
    "                            ious = compute_iou(pred_box, true_boxes)\n",
    "                            max_iou_idx = np.argmax(ious)\n",
    "                            if ious[max_iou_idx] > iou_threshold:\n",
    "                                all_labels.append(true_labels[max_iou_idx])\n",
    "                                all_preds.append(pred_label)\n",
    "                                matched_gt.add(max_iou_idx)\n",
    "                            else:\n",
    "                                all_labels.append(0)\n",
    "                                all_preds.append(pred_label)\n",
    "                        else:\n",
    "                            all_labels.append(0)\n",
    "                            all_preds.append(pred_label)\n",
    "\n",
    "                for i in range(len(true_boxes)):\n",
    "                    if i not in matched_gt:\n",
    "                        all_labels.append(true_labels[i])\n",
    "                        all_preds.append(0)\n",
    "\n",
    "    precision, recall, f1_score, _ = precision_recall_fscore_support(all_labels, all_preds, average='binary')\n",
    "\n",
    "    return precision, recall, f1_score\n",
    "\n",
    "# 使用訓練集進行評估\n",
    "precision, recall, f1_score = calculate_metrics(model, train_loader, device)\n",
    "print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 0.9856, Recall: 1.0000, F1-Score: 0.9928, mAP: 1.0000\n"
     ]
    }
   ],
   "source": [
    "def evaluate_model_performance(model, data_loader, device):\n",
    "    # 計算 mAP\n",
    "    precision, recall, f1_score, mAP = evaluate(model, data_loader, device)\n",
    "    print(f'Precision: {precision:.4f}, Recall: {recall:.4f}, F1-Score: {f1_score:.4f}, mAP: {mAP:.4f}')\n",
    "\n",
    "# 使用驗證集進行綜合性能評估\n",
    "evaluate_model_performance(model, train_loader, device)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
